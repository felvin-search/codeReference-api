{
  "r": {
    "apriori": "\"library(arules)\\ngroceries <- read.transactions(\\\"groceries.csv\\\", sep=\\\",\\\")\\nsummary(groceries)\\nitemFrequencyPlot(groceries, topN=20)\\n\\n#sample for randomly extracting samples, image function for visualing sparse matrix\\nimage(sample(groceries,100))\\ngroceries_rule <- apriori(data=groceries, parameter=list(support=0.006, confidence=0.25, minlen=2))\\nplotly_arules(groceries_rule)\\nsummary(groceries_rule)\\n\"",
    "decision tree": "\"library(rpart)\\nx <- cbind(x_train,y_train)\\n# grow tree \\nfit <- rpart(y_train ~ ., data = x,method=\\\"class\\\")\\nsummary(fit)\\n# Predict Output \\npredicted= predict(fit,x_test)\\n\"",
    "gradient boosting algorithms": "\"# GBM\\nlibrary(caret)\\nx <- cbind(x_train,y_train)\\n# Fitting model\\nfitControl <- trainControl( method = \\\"repeatedcv\\\", number = 4, repeats = 4)\\nfit <- train(y ~ ., data = x, method = \\\"gbm\\\", trControl = fitControl,verbose = FALSE)\\npredicted= predict(fit,x_test,type= \\\"prob\\\")[,2] \\n\\n\\n\\n# XGBoost\\nrequire(caret)\\nx <- cbind(x_train,y_train)\\n# Fitting model\\nTrainControl <- trainControl( method = \\\"repeatedcv\\\", number = 10, repeats = 4)\\nmodel<- train(y ~ ., data = x, method = \\\"xgbLinear\\\", trControl = TrainControl,verbose = FALSE)\\n# OR \\nmodel<- train(y ~ ., data = x, method = \\\"xgbTree\\\", trControl = TrainControl,verbose = FALSE)\\npredicted <- predict(model, x_test)\\n\\n\\n\\n# LightGBM\\nlibrary(RLightGBM)\\ndata(example.binary)\\n# Parameters\\nnum_iterations <- 100\\nconfig <- list(objective = \\\"binary\\\",  metric=\\\"binary_logloss,auc\\\", learning_rate = 0.1, num_leaves = 63, tree_learner = \\\"serial\\\", feature_fraction = 0.8, bagging_freq = 5, bagging_fraction = 0.8, min_data_in_leaf = 50, min_sum_hessian_in_leaf = 5.0)\\n# Create data handle and booster\\nhandle.data <- lgbm.data.create(x)\\nlgbm.data.setField(handle.data, \\\"label\\\", y)\\nhandle.booster <- lgbm.booster.create(handle.data, lapply(config, as.character))\\n# Train for num_iterations iterations and eval every 5 steps\\nlgbm.booster.train(handle.booster, num_iterations, 5)\\n# Predict\\npred <- lgbm.booster.predict(handle.booster, x.test)\\n# Test accuracy\\nsum(y.test == (y.pred > 0.5)) / length(y.test)\\n# Save model (can be loaded again via lgbm.booster.load(filename))\\nlgbm.booster.save(handle.booster, filename = \\\"/tmp/model.txt\\\")\\n\\n\\n\\n# Catboost\\nset.seed(1)\\n\\nrequire(titanic)\\n\\nrequire(caret)\\n\\nrequire(catboost)\\n\\ntt <- titanic::titanic_train[complete.cases(titanic::titanic_train),]\\n\\ndata <- as.data.frame(as.matrix(tt), stringsAsFactors = TRUE)\\n\\ndrop_columns = c(\\\"PassengerId\\\", \\\"Survived\\\", \\\"Name\\\", \\\"Ticket\\\", \\\"Cabin\\\")\\n\\nx <- data[,!(names(data) %in% drop_columns)]y <- data[,c(\\\"Survived\\\")]\\n\\nfit_control <- trainControl(method = \\\"cv\\\", number = 4,classProbs = TRUE)\\n\\ngrid <- expand.grid(depth = c(4, 6, 8),learning_rate = 0.1,iterations = 100, l2_leaf_reg = 1e-3,            rsm = 0.95, border_count = 64)\\n\\nreport <- train(x, as.factor(make.names(y)),method = catboost.caret,verbose = TRUE, preProc = NULL,tuneGrid = grid, trControl = fit_control)\\n\\nprint(report)\\n\\nimportance <- varImp(report, scale = FALSE)\\n\\nprint(importance)\\n\"",
    "knn": "\"library(knn)\\nx <- cbind(x_train,y_train)\\n# Fitting model\\nfit <-knn(y_train ~ ., data = x,k=5)\\nsummary(fit)\\n# Predict Output \\npredicted= predict(fit,x_test)\\n\"",
    "lasso": "\"data(ggplot2::diamonds)\\nlibrary(caret)\\nlibrary(dplyr)\\ndia.trans<-bind_cols(diamonds %>% select_if(is.numeric),\\n                     model.matrix(~cut-1,diamonds) %>% as_tibble(),\\n                     model.matrix(~color-1,diamonds) %>% as_tibble(),\\n                     model.matrix(~clarity-1,diamonds) %>% as_tibble())\\n\\n#setting parameters alpha and lambda\\nlasso_expand<-expand.grid(alpha = 1, lambda = seq(0.001,0.1,by = 0.0005))\\nlasso_mod <- train(x=dia.trans %>% select(-price), y=dia.trans$price, method='glmnet', \\n                   tuneGrid=lasso_expand)\\n\\n#best tune\\nlasso_mod$bestTune\\nlasso_mod$results$RMSE\\n\\nlasso_imp<-varImp(lasso_mod)\\n#get the importance of each feature and eliminate some of them\\nlasso_imp$importance\\n\"",
    "lightgbm": "\"library(RLightGBM)\\ndata(example.binary)\\n#Parameters\\n\\nnum_iterations <- 100\\nconfig <- list(objective = \\\"binary\\\",  metric=\\\"binary_logloss,auc\\\", learning_rate = 0.1, num_leaves = 63, tree_learner = \\\"serial\\\", feature_fraction = 0.8, bagging_freq = 5, bagging_fraction = 0.8, min_data_in_leaf = 50, min_sum_hessian_in_leaf = 5.0)\\n\\n#Create data handle and booster\\nhandle.data <- lgbm.data.create(x)\\n\\nlgbm.data.setField(handle.data, \\\"label\\\", y)\\n\\nhandle.booster <- lgbm.booster.create(handle.data, lapply(config, as.character))\\n\\n#Train for num_iterations iterations and eval every 5 steps\\n\\nlgbm.booster.train(handle.booster, num_iterations, 5)\\n\\n#Predict\\npred <- lgbm.booster.predict(handle.booster, x.test)\\n\\n#Test accuracy\\nsum(y.test == (y.pred > 0.5)) / length(y.test)\\n\\n#Save model (can be loaded again via lgbm.booster.load(filename))\\nlgbm.booster.save(handle.booster, filename = \\\"/tmp/model.txt\\\")\\n\"",
    "naive bayes": "\"library(e1071)\\nx <- cbind(x_train,y_train)\\n# Fitting model\\nfit <-naiveBayes(y_train ~ ., data = x)\\nsummary(fit)\\n# Predict Output \\npredicted= predict(fit,x_test)\\n\"",
    "random forest": "\"library(randomForest)\\nx <- cbind(x_train,y_train)\\n# Fitting model\\nfit <- randomForest(Species ~ ., x,ntree=500)\\nsummary(fit)\\n# Predict Output \\npredicted= predict(fit,x_test)\\n\"",
    "svm": "\"library(e1071)\\nx <- cbind(x_train,y_train)\\n# Fitting model\\nfit <-svm(y_train ~ ., data = x)\\nsummary(fit)\\n# Predict Output \\npredicted= predict(fit,x_test)\\n\"",
    "xgboost": "\"library(tidyverse)\\nlibrary(xgboost)\\n\\nind<-sample(2,nrow(diamonds),replace = T,prob = c(0.7,0.3))\\ntrain.set<-diamonds[ind==1,]\\ntest.set<-diamonds[ind==2,]\\n\\nxgb.train<-bind_cols(select_if(train.set,is.numeric),model.matrix(~cut-1,train.set) %>% as.tibble(),model.matrix(~color-1,train.set) %>% as.tibble(),model.matrix(~clarity-1,train.set) %>% as.tibble())\\nxgboost.train<-xgb.DMatrix(data = as.matrix(select(xgb.train,-price)),label=xgb.train$price)\\nxgb.test<-bind_cols(select_if(test.set,is.numeric),model.matrix(~cut-1,test.set) %>% as.tibble(),model.matrix(~color-1,test.set) %>% as.tibble(),model.matrix(~clarity-1,test.set) %>% as.tibble())\\nxgboost.test<-xgb.DMatrix(data = select(xgb.test,-price) %>% as.matrix(),label=xgb.test$price)\\n\\nparam<-list(eval_metric='rmse',gamma=1,max_depth=6,nthread = 3)\\nxg.model<-xgb.train(data = xgboost.train,params = param,watchlist = list(test=xgboost.test),nrounds = 500,early_stopping_rounds = 60,\\n                      print_every_n = 30)\\n                      \\nxg.predict<-predict(xg.model,xgboost.test)\\nmse.xgb<-sqrt(mean((test.set$price-xg.predict)^2))\\nplot((test.set$price-xg.predict))\\n\"",
    "dbscan clustering": "\"library(dbscan)\\ncl <- dbscan(iris[,-5], eps = .5, minPts = 5)\\nplot(iris[,-5], col = cl$cluster)\\n\"",
    "gmm": "\"library(mclust) # Gaussian mixture model (GMM)\\ngmm_fit <- Mclust(iris[, 1:4]) # Fit a GMM model\\nsummary(gmm_fit) # Summary table \\nplot(gmm_fit, 'BIC') # Select model based on BIC\\n\"",
    "heirarchical clustering": "\"clusters <- hclust(dist(iris[, -5]))\\nplot(clusters)\\n\"",
    "k-means": "\"library(cluster)\\nfit <- kmeans(X, 3) # 5 cluster solution\\n\"",
    "kmeans clustering": "\"cl <- kmeans(iris[,-5], 3)\\nplot(iris[,-5], col = cl$cluster)\\npoints(cl$centers, col = 1:3, pch = 8)\\n\"",
    "kmeans raw r": "\"custonKmeans<-function(dataset=NA,k=NA){\\n  if(is.na(dataset) || is.na(k)){\\n    stop(\\\"You must input valid parameters!\\\")\\n  }\\n  Eudist<-function(x,y){\\n    distance<-sqrt(sum((x-y)^2))\\n    return (distance)\\n  }\\n  \\n  rows.dataset<-nrow(dataset)\\n  continue.change=TRUE\\n  initPoint<-dataset[sample.int(rows.dataset,size = k),]\\n  formerPoint<-initPoint\\n  iterPoint<-matrix(0,nrow = k,ncol = ncol(dataset))\\n  \\n  #记录每一个点到每一个类的距离\\n  error.matrix<-matrix(0,nrow=rows.dataset,ncol=k)\\n  while(continue.change){\\n    #记录每个点所属的类是哪一个\\n    cluster.matrix<-matrix(0,nrow=rows.dataset,ncol=k)\\n    for(i in 1:rows.dataset){#计算每个点到三个初始中心点的距离\\n      for(j in 1:k){\\n        error.matrix[i,j]<-Eudist(dataset[i,],formerPoint[j,])\\n      }\\n    }\\n    #将每一个点所属的类计算出来\\n    for(i in 1:rows.dataset){\\n      cluster.matrix[i,which.min(error.matrix[i,])]<-1\\n    }\\n    \\n    #更新新的质心位置\\n    for(i in 1:k){\\n      iterPoint[i,]<-apply(dataset[which(cluster.matrix[,i] == 1),],2,\\\"mean\\\")\\n    }\\n    all.true<-c()\\n    for(i in 1:k){\\n      if(all(formerPoint[i,] == iterPoint[i,]) == T){\\n        all.true[i]<-TRUE\\n      }\\n    }\\n    formerPoint = iterPoint\\n    continue.change=ifelse(all(all.true) == T,F,T)\\n  }\\n  colnames(iterPoint)<-colnames(dataset)\\n  out=list()\\n  out[[\\\"centers\\\"]]<-iterPoint\\n  out[[\\\"distance\\\"]]<-error.matrix\\n  out[[\\\"cluster\\\"]]<-rep(1,rows.dataset)\\n  for(i in 1:rows.dataset){\\n    out[[\\\"cluster\\\"]][i]<-which(cluster.matrix[i,] == 1)\\n  }\\n  return(out)\\n}\\n\"",
    "pam": "\"library(cluster) \\npam_fit <- pam(iris[, 1:4], 5) # Partition Around Medoids\\nsummary(pam_fit) # Get summary\\n\"",
    "labelencode": "\"library(tidyverse)\\n#Divide data into train and test in 70% and 30%\\nind<-sample(2,nrow(diamonds),replace = T,prob = c(0.7,0.3))\\ntrain.set <- diamonds[ind==1,]\\ntest.set <- diamonds[ind==2,]\\n\\n#Combine the dataset using rbind function(inbuilt function)\\ncombi <- rbind(train.set, test.set)\\n\\n##Label Encoding\\ncombi[, cut_num := ifelse(cut == \\\"Fair\\\",0,\\n                                   ifelse(cut == \\\"Good\\\",1,\\n                                   ifelse(cut == \\\"Very Good\\\",2,\\n                                   ifelse(cut == \\\"Premium\\\",3,4))))]\\ncombi[, color_num := ifelse(color == \\\"D\\\",0,\\n                                   ifelse(color == \\\"E\\\",2,\\n                                   ifelse(color == \\\"F\\\",3,\\n                                   ifelse(color == \\\"G\\\",4,\\n                                   ifelse(color == \\\"H\\\",5,\\n                                   ifelse(color == \\\"I\\\",6,7))))))]\\n\\n# Column \\\"clarity\\\" won't be taken in label encoding as it contains more variables.\\n#The more variables in column in label encoding, the model will perform less.\\n\\n#Removing categorical variables after label encoding\\ncombi[,c(\\\"color\\\", \\\"cut\\\") := NULL)\\n                                                     \\n#Divide data back into train and test in 70% and 30%\\nind<-sample(2,nrow(combi),replace = T,prob = c(0.7,0.3))\\ntrain.set <- combi[ind==1,]\\ntest.set <- combi[ind==2,]\\n\"",
    "onehotencode": "\"oneHotEncode <- function(x, fullRank = T){\\n    if(fullRank){\\n        return(model.matrix(~ 0 + ., data = x))\\n    } else {\\n        charCols <- colnames(x)[sapply(x, is.character)]\\n        if(length(charCols) > 0){\\n            for(col in charCols){\\n                x[[eval(col)]] <- factor(x[[eval(col)]])\\n            }\\n        }\\n        factorCols <- colnames(x)[sapply(x, is.factor)]\\n        contrastsList <- vector(mode = \\\"list\\\", length = length(factorCols))\\n        names(contrastsList) <- factorCols\\n        if(length(factorCols) > 0){\\n            for(col in factorCols){\\n                contrastsList[[eval(col)]] <- contrasts(x[[eval(col)]], contrasts = F)\\n            }\\n            return(model.matrix(~ 0 + ., data = x, contrasts = contrastsList))\\n        } else {\\n            return(model.matrix(~ 0 + ., data = x))\\n        }\\n    }\\n}\\n\\ndiamonds <- ggplot2::diamonds\\nhead(oneHotEncode(diamonds))\\nhead(oneHotEncode(diamonds, fullRank = F))\\n\"",
    "data normalization standardization": "\"# normalization & standardization\\nnormalization<-function(x){\\n  return((x-min(x))/(max(x)-min(x)))\\n}\\n\\nstandardization<-function(x){\\n  return((x-mean(x))/sd(x))\\n}\\n\\nhead(iris)\\n# Sepal.Length Sepal.Width Petal.Length Petal.Width Species\\n# 1          5.1         3.5          1.4         0.2  setosa\\n# 2          4.9         3.0          1.4         0.2  setosa\\n# 3          4.7         3.2          1.3         0.2  setosa\\n# 4          4.6         3.1          1.5         0.2  setosa\\n# 5          5.0         3.6          1.4         0.2  setosa\\n# 6          5.4         3.9          1.7         0.4  setosa\\n\\niris<-iris[,-5]\\nhead(iris)\\n# Sepal.Length Sepal.Width Petal.Length Petal.Width\\n# 1          5.1         3.5          1.4         0.2\\n# 2          4.9         3.0          1.4         0.2\\n# 3          4.7         3.2          1.3         0.2\\n# 4          4.6         3.1          1.5         0.2\\n# 5          5.0         3.6          1.4         0.2\\n# 6          5.4         3.9          1.7         0.4\\n\\n#normalize\\napply(as.matrix(iris),2,normalization)\\n# Sepal.Length Sepal.Width Petal.Length Petal.Width\\n# [1,]   0.22222222  0.62500000   0.06779661  0.04166667\\n# [2,]   0.16666667  0.41666667   0.06779661  0.04166667\\n# [3,]   0.11111111  0.50000000   0.05084746  0.04166667\\n# [4,]   0.08333333  0.45833333   0.08474576  0.04166667\\n# [5,]   0.19444444  0.66666667   0.06779661  0.04166667\\n# [6,]   0.30555556  0.79166667   0.11864407  0.12500000\\n# [7,]   0.08333333  0.58333333   0.06779661  0.08333333\\n\\n#standardize\\napply(as.matrix(iris),2,standardization)\\n# Sepal.Length Sepal.Width Petal.Length   Petal.Width\\n# [1,]  -0.89767388  1.01560199  -1.33575163 -1.3110521482\\n# [2,]  -1.13920048 -0.13153881  -1.33575163 -1.3110521482\\n# [3,]  -1.38072709  0.32731751  -1.39239929 -1.3110521482\\n# [4,]  -1.50149039  0.09788935  -1.27910398 -1.3110521482\\n# [5,]  -1.01843718  1.24503015  -1.33575163 -1.3110521482\\n# [6,]  -0.53538397  1.93331463  -1.16580868 -1.0486667950\\n# [7,]  -1.50149039  0.78617383  -1.33575163 -1.1798594716\"",
    "data processing": "\"library(xlsx)\\n## Loading required package: rJava\\n## Loading required package: xlsxjars\\n\\nsetwd(\\\"/Users/chenfeiyang\\\")\\ncameraData <- read.xlsx(\\\"./data/cameras.xlsx\\\", sheetIndex = 1, header = TRUE)\\ncameraData <- read.xlsx(\\\"./data/cameras.xlsx\\\", \\\"Baltimore Fixed Speed Cameras\\\", \\n    header = TRUE)\\nhead(cameraData)\\n##                          address direction      street  crossStreet\\n## 1       S CATON AVE & BENSON AVE       N/B   Caton Ave   Benson Ave\\n## 2       S CATON AVE & BENSON AVE       S/B   Caton Ave   Benson Ave\\n## 3 WILKENS AVE & PINE HEIGHTS AVE       E/B Wilkens Ave Pine Heights\\n## 4        THE ALAMEDA & E 33RD ST       S/B The Alameda      33rd St\\n## 5        E 33RD ST & THE ALAMEDA       E/B      E 33rd  The Alameda\\n## 6        ERDMAN AVE & N MACON ST       E/B      Erdman     Macon St\\n##                 intersection                      Location.1\\n## 1     Caton Ave & Benson Ave (39.2693779962, -76.6688185297)\\n## 2     Caton Ave & Benson Ave (39.2693157898, -76.6689698176)\\n## 3 Wilkens Ave & Pine Heights  (39.2720252302, -76.676960806)\\n## 4     The Alameda  & 33rd St (39.3285013141, -76.5953545714)\\n## 5      E 33rd  & The Alameda (39.3283410623, -76.5953594625)\\n## 6         Erdman  & Macon St (39.3068045671, -76.5593167803)\\n\\n# Read specific rows and columns in Excel\\ncolIndex <- 2:3\\nrowIndex <- 1:4\\ncameraDataSubset <- read.xlsx(\\\"./data/cameras.xlsx\\\", sheetIndex = 1, colIndex = colIndex, \\n    rowIndex = rowIndex)\\ncameraDataSubset\\n##   direction      street\\n## 1       N/B   Caton Ave\\n## 2       S/B   Caton Ave\\n## 3       E/B Wilkens Ave\\n\\n# Subsetting - quick review\\nset.seed(13435)\\nX <- data.frame(var1 = sample(1:5), var2 = sample(6:10), var3 = sample(11:15))\\nX <- X[sample(1:5), ]\\nX$var2[c(1, 3)] = NA\\nX\\n##   var1 var2 var3\\n## 1    2   NA   15\\n## 4    1   10   11\\n## 2    3   NA   12\\n## 3    5    6   14\\n## 5    4    9   13\\n\\nX[, 1]\\n## [1] 2 1 3 5 4\\nX[, \\\"var1\\\"]\\n## [1] 2 1 3 5 4\\nX[1:2, \\\"var2\\\"]\\n## [1] NA 10\\n\\n# Logicals and: & , or: |\\nX[(X$var1 <= 3 & X$var3 > 11), ]\\n##   var1 var2 var3\\n## 1    2   NA   15\\n## 2    3   NA   12\\nX[(X$var1 <= 3 | X$var3 > 15), ]\\n##   var1 var2 var3\\n## 1    2   NA   15\\n## 4    1   10   11\\n## 2    3   NA   12\\n\\n## Dealing with missing values\\nX[which(X$var2 > 8), ]\\n##   var1 var2 var3\\n## 4    1   10   11\\n## 5    4    9   13\\n\\n# Sorting\\nsort(X$var1)\\n## [1] 1 2 3 4 5\\nsort(X$var1, decreasing = TRUE)\\n## [1] 5 4 3 2 1\\nsort(X$var2, na.last = TRUE)\\n## [1]  6  9 10 NA NA\\n\\n# Ordering\\nX[order(X$var1), ]\\n##   var1 var2 var3\\n## 4    1   10   11\\n## 1    2   NA   15\\n## 2    3   NA   12\\n## 5    4    9   13\\n## 3    5    6   14\\n\\nX[order(X$var1, X$var3), ]\\n##   var1 var2 var3\\n## 4    1   10   11\\n## 1    2   NA   15\\n## 2    3   NA   12\\n## 5    4    9   13\\n## 3    5    6   14\\n\\n## Sort using the arrange function of the plyr package\\n\\nlibrary(plyr)\\narrange(X, var1)\\n##   var1 var2 var3\\n## 1    1   10   11\\n## 2    2   NA   15\\n## 3    3   NA   12\\n## 4    4    9   13\\n## 5    5    6   14\\n\\narrange(X, desc(var1))\\n##   var1 var2 var3\\n## 1    5    6   14\\n## 2    4    9   13\\n## 3    3   NA   12\\n## 4    2   NA   15\\n## 5    1   10   11\\n\\n# Add row and column\\nX$var4 <- rnorm(5)\\nX\\n##   var1 var2 var3     var4\\n## 1    2   NA   15  0.18760\\n## 4    1   10   11  1.78698\\n## 2    3   NA   12  0.49669\\n## 3    5    6   14  0.06318\\n## 5    4    9   13 -0.53613\\n\\nY <- cbind(X, rnorm(5))\\nY\\n##   var1 var2 var3     var4 rnorm(5)\\n## 1    2   NA   15  0.18760  0.62578\\n## 4    1   10   11  1.78698 -2.45084\\n## 2    3   NA   12  0.49669  0.08909\\n## 3    5    6   14  0.06318  0.47839\\n## 5    4    9   13 -0.53613  1.00053\\n\"",
    "dimensionality reduction algorithms": "\"library(stats)\\npca <- princomp(train, cor = TRUE)\\ntrain_reduced  <- predict(pca,train)\\ntest_reduced  <- predict(pca,test)\\n\"",
    "k folds": "\"# K folds cross validation is essential for machine learning\\n# createFolds function in package caret is easy to use\\n# here we write our own function\\n\\nget_k_folds<-function(y = c(),k = 10, isList = TRUE, seed = 123){\\n  set.seed(seed)\\n  folds<-sample(1:length(y), length(y))\\n  every_n<-ceiling(length(y)/k)\\n  matFolds<-suppressWarnings(matrix(folds, ncol=every_n, byrow = T))\\n  \\n  if(isList){\\n    value<-NULL\\n    rownames(matFolds)<-paste(\\\"Folds\\\",1:k,sep=\\\"\\\")\\n    value<-lapply(1:k, function(x){\\n      if(x == k){\\n        return(matFolds[x,][1:(length(y)-every_n*(k-1))])\\n      }else{\\n        return(matFolds[x,])\\n      }\\n    })\\n  }else{\\n    value<-c()\\n    for(i in 1:length(y)){\\n      value[i]<-ceiling(i/every_n)\\n    }\\n  }\\n  \\n  return(value)\\n}\\n\"",
    "euclideandistance": "\"euclideanDistance <- function(x, y) {\\n    return(sqrt(sum((x - y)^2)))\\n}\\n\\nset.seed(1)\\nx <- rnorm(1000)\\ny <- runif(1000)\\nprint(euclideanDistance(x, y))\"",
    "factorial": "\"Fact <- function(n){\\n  if(n < 0){\\n    stop(\\\"Error: your input is wrong!\\\")\\n  } else if(n == 0){\\n    return(1)\\n  } else {\\n    return(prod(1:n))\\n  }\\n}\\n\\nFact(5)\\nFact(6)\\n\"",
    "fibonacci": "\"Fibonacci <- function(n)\\n{\\n  if(n == 1|n == 2)\\n  {\\n    return(1)\\n  }\\n  else\\n  {\\n    return(Fibonacci(n-1) + Fibonacci(n - 2))\\n  }\\n}\\n\\nFibonacci(1)\\nFibonacci(11)\\n\"",
    "perfectsquare": "\"perfectSquare <- function(x){\\n    return(floor(sqrt(x)) == sqrt(x))\\n}\\n\\nset.seed(1)\\ninputs <- sample(1:100, 10)\\nperfectSquare(inputs)\"",
    "pimontecarlo": "\"estimatePi <- function(numSims){\\n    x <- runif(numSims)\\n    y <- runif(numSims)\\n    inUnitCircle <- as.integer(x^2 + y^2 <= 1)\\n    return(4 * sum(inUnitCircle) / numSims)\\n}\\n\\nset.seed(1)\\nestimatePi(3000)\\nestimatePi(30000)\\n\"",
    "prime": "\"# Prime Number Checking in R\\nisPrime <- function(number) {\\n  if (number == 2 | number == 3) {\\n    return(TRUE)\\n  } else if (number %% 2 == 0 | number %% 3 == 0){\\n    return(FALSE)\\n  } else {\\n    k <- 1\\n    while(6 * k - 1 <= sqrt(number)){\\n      if(number %% (6 * k + 1) == 0){\\n        return(FALSE)\\n      } else if(number %% (6 * k - 1) == 0){\\n        return(FALSE)\\n      }\\n      k <- k + 1\\n    }\\n    return(TRUE)\\n  }\\n}\\n\\nisPrime(2)\\nisPrime(5)\\nisPrime(4)\\n\"",
    "ann": "\"library(neuralnet)\\nconcrete<-read.csv(file = \\\"concrete.txt\\\",stringsAsFactors = F)#get the data\\nnormalize<-function(x){\\n  return((x-min(x))/(max(x)-min(x)))\\n}\\nconcrete<-as.data.frame(lapply(concrete, normalize))\\nconcrete_train<-concrete[1:773,]\\nconcrete_test<-concrete[774:1030,]\\nconcrete_model<-neuralnet(strength~cement+slag+ash+water+superplastic+coarseagg+fineagg+age,data = concrete_train,hidden = 5)\\nmodel_res<-compute(concrete_model,concrete_test[,1:8])\\nx=model_res$net.result\\ny=concrete_test$strength\\ncor(x,y)\\nplot(concrete_model)\\n\"",
    "linearregressionrawr": "\"ols<-function(y,x){\\n    data<-model.matrix(y ~ ., data = x)\\n    decomp <- svd(data)\\n    return(decomp$v %*% diag(1 / decomp$d) %*% t(decomp$u) %*% y)\\n  }\\n\\nset.seed(1)\\nx <- rnorm(1000)\\ny <- 4 * x + rnorm(1000, sd = .5)\\nols(y=y,x=matrix(x, ncol = 1))\\n\"",
    "linear regression": "\"# Load Train and Test datasets\\n# Identify feature and response variable(s) and values must be numeric and numpy arrays\\nx_train <- input_variables_values_training_datasets\\ny_train <- target_variables_values_training_datasets\\nx_test <- input_variables_values_test_datasets\\nx <- cbind(x_train,y_train)\\n# Train the model using the training sets and check score\\nlinear <- lm(y_train ~ ., data = x)\\nsummary(linear)\\n# Predict Output\\npredicted= predict(linear,x_test) \\n\"",
    "logistic regression": "\"x <- cbind(x_train,y_train)\\n# Train the model using the training sets and check score\\nlogistic <- glm(y_train ~ ., data = x,family='binomial')\\nsummary(logistic)\\n# Predict Output\\npredicted= predict(logistic,x_test)\\n\"",
    "logistic regression2": "\"# Introduction to logistic regression\\n\\n# glm stands for Generalized Linear Model\\nmod1 <- glm(y_data~x_data, data=name_of_the_dataframe, family=\\\"binomial\\\")\\n\\n# displays the output of the model computed by the previous line\\nsummary(mod1)\\n\\n# modeled data : it predicts the output for x_test_data as input information for the model\\npredicted <- predict(mod1, x_test_data)\\n\"",
    "multiple linear regression": "\"# Introduction to multiple linear regression\\n\\n# lm stands for Linear Model\\n# y_data are modeled as a.x1 + b.x2 + c.x3 + d.x4 + e\\nmod3 <- lm(y_data~x1_data+x2_data+x3_data+x4_data, data=name_of_the_dataframe)\\n\\n# displays the output of the model computed by the previous line\\nsummary(mod3)\\n\\n# modeled data : it predicts the output for x_test_data as input information for the model\\npredicted <- predict(mod3, x1_test_data, x2_test_data, x3_test_data, x4_test_data)\\n\"",
    "bubble sort": "\"# Bubble sort in R:\\n\\nbubble.sort <- function(elements.vec) { \\n  n <- length(elements.vec)\\n  for(i in 1:(n-1)) {\\n    for(j in 1:(n-i)) {\\n      if(elements.vec[j+1] < elements.vec[j]) { \\n        temp <- elements.vec[j]\\n        elements.vec[j] <- elements.vec[j+1]\\n        elements.vec[j+1] <- temp\\n      }\\n    }\\n  }\\n  return(elements.vec)\\n}\\n\\n# Example:\\n# bubble.sort(c(5, 2, 3, 1, 4))\\n# [1] 1 2 3 4 5\\n\"",
    "comb sort": "\"# Comb sort in R:\\n\\ncomb.sort <- function(elements.vec) {\\n  gap <- length(elements.vec)\\n  swaps <- 1\\n  while (gap > 1 && swaps == 1) {\\n    gap = floor(gap / 1.3)\\n    if (gap < 1) {\\n      gap = 1\\n    }\\n    swaps = 0\\n    i = 1\\n    while (i + gap <= length(a)) {\\n      if (elements.vec[i] > elements.vec[i + gap]) {\\n        elements.vec[c(i, i + gap)] <- elements.vec[c(i + gap, i)]\\n        swaps = 1\\n      }\\n      i <- i + 1\\n    }\\n  }  \\n  return(elements.vec) \\n}\\n\\n# Example:\\n# comb.sort(sample(1:100,10))\\n# [1] 9 49 50 51 56 60 61 71 86 95\\n\"",
    "heap sort": "\"# Heap sort in R:\\n\\nbuild.heap <- function(elements.vec) {\\n  l = length(elements.vec)\\n  heap = elements.vec\\n  for (i in l:1) {\\n    heap = modify.heap(heap, i)\\n  }\\n  return(heap)\\n}\\n\\nis.heap <- function(heap, rootIndex) {\\n  i = rootIndex\\n  res = T\\n  while(2 * i <= length(heap) & res) {\\n    child = c(heap[2 * i], heap[2 * i + 1])\\n    child = child[!is.na(child)]\\n    result.bool = all(heap[i] <= child)\\n    i = i + 1\\n  }\\n  return(result.bool)\\n}\\n\\nmodify.heap <- function(heap, rootIndex) {\\n  l = length(heap)\\n  flag = 1  \\n  while (rootIndex * 2 <= l && flag == 1) {\\n    leftIndex = rootIndex * 2\\n    rightIndex = rootIndex * 2 + 1\\n    flag = 0\\n    child = c(heap[leftIndex], heap[rightIndex])\\n    child = child[!is.na(child)]\\n    minIndex = which.min(child)\\n    if (heap[rootIndex] > child[minIndex]) {\\n      flag = 1\\n      heapIndex = c(leftIndex, rightIndex)[minIndex]\\n      temp = heap[heapIndex]\\n      heap[heapIndex] = heap[rootIndex]\\n      heap[rootIndex] = temp\\n      rootIndex = heapIndex\\n    }\\n  }\\n  return(heap)\\n}\\n\\nheap.sort <- function(heap) {\\n  sorted.elements = NULL\\n  l = length(heap)\\n  while(l > 0)\\n  {\\n    sorted.elements = c(sorted.elements, heap[1])\\n    l = length(heap)\\n    heap[1] = heap[l]\\n    heap = heap[1:(l - 1)]\\n    heap = modify.heap(heap, rootIndex = 1)\\n    l = l - 1\\n  }\\n  return(sorted.elements)\\n}\\n\\n# Example:\\n# heap.sort(build.heap(c(5, 2, 3, 1, 4))) \\n# [1] 1 2 3 4 5\\n\"",
    "insertion sort": "\"# Insertion sort in R:\\n\\ninsertion.sort <- function(elements.vec) { \\n  for (j in 2:length(elements.vec)) {\\n    key = elements.vec[j] \\n    i = j - 1\\n    while (i > 0 && elements.vec[i] > key) {\\n      elements.vec[(i + 1)] = elements.vec[i]\\n      i = i - 1\\n    }\\n    elements.vec[(i + 1)] = key\\n  }\\n  return(elements.vec)\\n}\\n\\n# Example:\\n# insertion.sort(c(5, 2, 3, 1, 4))\\n# [1] 1 2 3 4 5\\n\"",
    "merge sort": "\"# Merge sort in R:\\n\\nmerge.func <-function(leftArray, rightArray) {\\n    l <- numeric(length(leftArray) + length(rightArray))\\n    leftIndex <- 1; rightIndex <- 1; i <- 1;\\n    for(i in 1:length(l)) {\\n        if((leftIndex <= length(leftArray) && leftArray[leftIndex] < rightArray[rightIndex]) || rightIndex > length(rightArray)) {\\n            l[i] <- leftArray[leftIndex]\\n            leftIndex <- leftIndex + 1\\n        } else {\\n            l[i] <- rightArray[rightIndex]\\n            rightIndex <- rightIndex + 1\\n        }\\n    }\\n    return(l)\\n}\\n\\nmerge.sort <- function(elements.vec) {\\n    if(length(elements.vec) > 1) { \\n        m <- ceiling(length(elements.vec) / 2)\\n        leftArray <- merge.sort(elements.vec[1:m])\\n        rightArray <- merge.sort(elements.vec[(m + 1):length(elements.vec)])\\n        merge.func(leftArray, rightArray)\\n    } \\n    else {\\n        return(elements.vec)\\n    }\\n}\\n\\n# Example:\\n# merge.sort(c(5, 2, 3, 1, 4)) \\n# [1] 1 2 3 4 5\\n\"",
    "quick sort": "\"# Quick sort in R:\\n\\nquick.sort <- function(elements.vec) {\\n  if(length(elements.vec) <= 1) {\\n    return(elements.vec)\\n  }\\n  pivot <- elements.vec[1]\\n  non.pivot  <- elements.vec[-1]\\n  pivot_less    <- quick.sort(non.pivot[non.pivot < pivot])\\n  pivot_greater <- quick.sort(non.pivot[non.pivot >= pivot])\\n  return(c(pivot_less, pivot, pivot_greater))\\n}\\n\\n# Example:\\n# quick.sort(c(5, 2, 3, 1, 1, 4)) \\n# [1] 1 1 2 3 4 5\\n\\n# Notes:\\n# 1. Quick sort is not a stable sorting algorithm.\\n# 2. It is implemented in the 'sort' function of base R:\\n# sort(c(5, 2, 3, 1, 1, 4), method = \\\"quick\\\" , index.return = FALSE)\\n# [1] 1 1 2 3 4 5\\n\"",
    "radix sort": "\"# Radix sort in R:\\n\\nradix.sort <- function(elements.vec) {\\n    x <- nchar(max(elements.vec))\\n    for (i in 1:x)\\n        elements.vec <- elements.vec[order(elements.vec %% (10 ^ i))]\\n    return(elements.vec)\\n}\\n\\n# Example:\\n# radix.sort(c(50, 3200, 27, 976, 820)) \\n# [1] 27 50 820 976 3200\\n\\n# Note:\\n# It is implemented in the 'sort' function of base R:\\n# sort(c(50, 3200, 27, 976, 820), method = \\\"radix\\\" , index.return = FALSE)\\n# [1] 27 50 820 976 3200\\n\"",
    "selection sort": "\"# Selection sort in R:\\n\\nselection.sort <- function(elements.vec, ascending = TRUE) {\\n  max <- length(elements.vec)\\n  for (j in 1:(max - 1)) {\\n    m <- elements.vec[j]\\n    p <- j\\n    for(k in (j + 1):max) {\\n      if(ascending && elements.vec[k] < m || !ascending && elements.vec[k] > m) {\\n        m <- elements.vec[k]\\n        p <- k\\n      }\\n    } \\n    elements.vec[p] <- elements.vec[j]\\n    elements.vec[j] <- m\\n  } \\n  return(elements.vec)\\n}\\n\\n# Example:\\n# selection.sort(c(5, 2, 3, 1, 1, 4)) \\n# [1] 1 1 2 3 4 5\\n# Note that selection sort is not a stable sorting algorithm.\\n\"",
    "stooge sort": "\"# Stooge sort in R:\\n\\nstooge.sort <- function(elements.vec) {\\n\\ti = 1\\n\\tj = length(elements.vec)\\n\\tif (elements.vec[j] < elements.vec[i]) elements.vec[c(j, i)] = elements.vec[c(i, j)]\\n\\tif (j - i > 1) {\\n\\t\\tt = (j - i + 1) %/% 3\\n\\t\\telements.vec[i:(j - t)] = stooge.sort(elements.vec[i:(j - t)])\\n\\t\\telements.vec[(i + t):j] = stooge.sort(elements.vec[(i + t):j])\\n\\t\\telements.vec[i:(j - t)] = stooge.sort(elements.vec[i:(j - t)])\\n\\t}\\n\\telements.vec\\n}\\n \\n# Example:\\n# stooge.sort(sample(21, 20))\\n# [1] 1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21\\n\""
  }
}
